{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset pre-prossesing\n",
    "#Step 1: Read csv file and create a dataframe\n",
    "#Step 2: Save dataframes into pkl files\n",
    "#Step 3: Pre-processing \n",
    "#Step 4: Save pre-processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import re\n",
    "import spacy\n",
    "import unicodedata\n",
    "import greek_stemmer\n",
    "import emoji\n",
    "import spacy\n",
    "import el_core_news_sm\n",
    "import string \n",
    "import numpy as np\n",
    "from greek_stemmer import GreekStemmer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "string.punctuation\n",
    "nlp = el_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation = \"AllData.csv\"\n",
    "\n",
    "c_l = []\n",
    "everything = []\n",
    "\n",
    "chunksize = 100\n",
    "    \n",
    "for chunk in pd.read_csv(annotation,engine='python',encoding = 'utf8',sep=\";\",header=1, usecols = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], chunksize=chunksize):\n",
    "    \n",
    "    r = chunk\n",
    "    r.columns = [\"Channel\",\"VideoTitle\",\"VideoLink\",\"VideoPublishedYear\",\"VideoViews\",\"CommentID\",\"ParrentCommentID\",\"CommentAuthor\",\"CommentText\",\"CommentYear\",\"Label\"]\n",
    "#     print(r.CommentText, r.Label)\n",
    "    for i in range(len(chunk)):\n",
    "        c_l.append({'CommentText': r.iloc[i]['CommentText'], 'Label': r.iloc[i]['Label']})\n",
    "        everything.append({'Channel': r.iloc[i]['Channel'],'VideoTitle': r.iloc[i]['VideoTitle'],'VideoLink': r.iloc[i]['VideoLink'],'VideoPublishedYear': r.iloc[i]['VideoPublishedYear'],'VideoViews': r.iloc[i]['VideoViews'],'CommentID': r.iloc[i]['CommentID'],'ParrentCommentID': r.iloc[i]['ParrentCommentID'],'CommentAuthor': r.iloc[i]['CommentAuthor'],'CommentText': r.iloc[i]['CommentText'],'CommentYear': r.iloc[i]['CommentYear'],'Label': r.iloc[i]['Label']})\n",
    "        \n",
    "Commnet_Label = pd.DataFrame(c_l)\n",
    "All = pd.DataFrame(everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Commnet_Label = pd.DataFrame(c_l)\n",
    "All = pd.DataFrame(everything)\n",
    "\n",
    "with open('Commnet_Label.pkl', 'wb') as handle:\n",
    "    pkl.dump(Commnet_Label, handle)\n",
    "    \n",
    "with open('All.pkl', 'wb') as handle:\n",
    "    pkl.dump(All, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('Commnet_Label.pkl', 'rb') as handle:\n",
    "    Comment_Label = pkl.load(handle)\n",
    "    \n",
    "with open('All.pkl', 'rb') as handle:\n",
    "    All = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Explicit        7638\n",
       "Implicit        5459\n",
       "No hate         4113\n",
       "Can’t define    1152\n",
       "Noise            874\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Comment_Label.Label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataframe with only Explict and No Hate comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Expicit_NoHate = Comment_Label.loc[(Comment_Label['Label'] == \"No hate\") | (Comment_Label['Label'] == \"Explicit\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove links, \\n char \\ufeff, @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate.reset_index(drop=True, inplace=True)\n",
    "\n",
    "NoLinks = Expicit_NoHate['CommentText'].str.replace('http\\S+|www.\\S+', '', case=False)\n",
    "Expicit_NoHate['NoLink'] = NoLinks\n",
    "\n",
    "#Remove \\n char \\ufeff, @\n",
    "\n",
    "Expicit_NoHate['NoLink'] = Expicit_NoHate['NoLink'].str.replace('\\n', '', case=False)\n",
    "Expicit_NoHate['NoLink'] = Expicit_NoHate['NoLink'].str.replace('\\ufeff', '', case=False)\n",
    "Expicit_NoHate['NoLink'] = Expicit_NoHate['NoLink'].str.strip()\n",
    "\n",
    "Expicit_NoHate['NoAuthor']= Expicit_NoHate['NoLink'].str.replace(r'^@', '', case=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_usernames(df_text,df_users):\n",
    "    new_df=[]\n",
    "\n",
    "    unique_authors = df_users.unique()\n",
    "    unique_authors = list(unique_authors)\n",
    "    unique_authors.sort(key = len, reverse=True)\n",
    "\n",
    "    for i, x in enumerate(df_text):\n",
    "\n",
    "\n",
    "        for author in unique_authors:\n",
    "            x=x.replace(author, '')\n",
    "        new_df.append(x)\n",
    "\n",
    "    obj_df = pd.DataFrame(new_df)\n",
    "\n",
    "    return obj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate['NoAuthor'] = delete_usernames(Expicit_NoHate['NoAuthor'],All[\"CommentAuthor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create new column with emojie name and count dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emojie(df):\n",
    "    new_df=[]\n",
    "    \n",
    "    for row in df:\n",
    "        emojie=[]\n",
    "        \n",
    "        \n",
    "        for a in emoji.emoji_lis(row):\n",
    "                    \n",
    "                emojie_text = emoji.demojize(a[\"emoji\"])\n",
    "                emojie.append(emojie_text)\n",
    "                \n",
    "                \n",
    "        if not emojie:  \n",
    "            new_df.append(None)\n",
    "        else:\n",
    "            emojie=dict(Counter(emojie))\n",
    "            new_df.append(emojie)\n",
    "            \n",
    "            \n",
    "    obj_df = pd.DataFrame(new_df)\n",
    "    return obj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Emojie\"] = emojie(Expicit_NoHate[\"NoAuthor\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Upper\"] = Expicit_NoHate[\"NoAuthor\"].str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove whitespace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Preprocessed\"]=Expicit_NoHate[\"Upper\"].str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#link github : https://gist.github.com/rcalsaverini/30bb8212809d29592222\n",
    "\n",
    "def strip_accents(unicode_string):\n",
    "\n",
    "    ndf_string = unicodedata.normalize('NFD', unicode_string)\n",
    "    is_not_accent = lambda char: unicodedata.category(char) != 'Mn'\n",
    "    return ''.join(\n",
    "        char for char in ndf_string if is_not_accent(char) \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Upper\"]=Expicit_NoHate[\"Upper\"].apply(strip_accents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoPunctuation(s):\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    s = ''.join(ch for ch in s if ch not in exclude)\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greeklish to Greek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conversion_pool():\n",
    "    poolGR = u\"ΑΒΓΔΕΖΗΘΙΚΛΜΝΞΟΠΡΣΤΥΦΧΨΩΥΚΒ\"\n",
    "    poolGL =  \"ABGDEZH8IKLMN3OPRSTUFX4WYCV\"\n",
    "    return dict(zip(poolGL, poolGR))\n",
    "\n",
    "def convert(st):\n",
    "    pool = get_conversion_pool()\n",
    "    r=''\n",
    "    \n",
    "    s = u'{}'.format(st)\n",
    "    s = s.replace('THN',u'ΤΗΝ')\n",
    "    s = s.replace('THS',u'ΤΗΣ')\n",
    "    s = s.replace('THA',u'ΘΑ')\n",
    "    s = s.replace('J',u'TZ')\n",
    "    s = s.replace('HA',u'ΧΑ')\n",
    "    s = s.replace('KS',u'Ξ')\n",
    "    s = s.replace('TH',u'Θ')\n",
    "    s = s.replace('PS',u'Ψ')\n",
    "    \n",
    "\n",
    "    for i in range (len(s)):\n",
    "        line = s[i]\n",
    "        output_line = []\n",
    "        \n",
    "        for character in line:\n",
    "        \n",
    "            if character in pool:\n",
    "                output_line.append(pool[character])\n",
    "            else:\n",
    "                output_line.append(character)\n",
    "\n",
    "        r = r + \"\".join(output_line)\n",
    "    return(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Greek\"]=Expicit_NoHate[\"Upper\"].apply(convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    r=''\n",
    "    for token in doc:\n",
    "        output_line = []\n",
    "        if token.is_stop:\n",
    "\n",
    "            output_line.append(\"\") \n",
    "\n",
    "\n",
    "        else:\n",
    "    #         print(token)\n",
    "            token=str(token)\n",
    "            output_line.append(token+' ')\n",
    "\n",
    "        r = r + \" \".join(output_line)\n",
    "    return(re.sub(' +', ' ',r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"NoStopwords\"]=Expicit_NoHate[\"Greek\"].apply(r_stopwords)\n",
    "lower_no_stop = Expicit_NoHate[\"NoStopwords\"].str.lower()\n",
    "Expicit_NoHate[\"NoStopwords\"] = lower_no_stop.apply(r_stopwords)\n",
    "Expicit_NoHate['NoStopwords'] = Expicit_NoHate['NoStopwords'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate.NoStopwords = Expicit_NoHate.NoStopwords.str.upper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    doc = nlp(text)\n",
    "    return[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"Tokens\"] = Expicit_NoHate[\"NoStopwords\"].apply(tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def POS(text):\n",
    "    \n",
    "    \n",
    "    l=[]\n",
    "    text = text.lower()\n",
    "    doc = nlp(text)\n",
    "\n",
    "    for word in doc:\n",
    "        if emoji.emoji_lis(str(word)):\n",
    "            pos = emoji.demojize(str(word))\n",
    "\n",
    "        else:\n",
    "            pos = str(word.pos_)\n",
    "        l.append(pos)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate['POS'] = Expicit_NoHate.NoStopwords.apply(POS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GR_stemmer(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    stemmer = GreekStemmer()\n",
    "    r=''\n",
    "    \n",
    "    for token in doc:\n",
    "        output_line = []\n",
    "        token = str(token)\n",
    "        stemm = stemmer.stem(token)\n",
    "        \n",
    "\n",
    "        output_line.append(stemm+' ')\n",
    "\n",
    "        r = r + \" \".join(output_line)\n",
    "    return(re.sub(' +', ' ',r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate['Stemming'] =  Expicit_NoHate.NoStopwords.apply(GR_stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laugh(text):\n",
    "\n",
    "    r=''\n",
    "    \n",
    "    regex = re.compile('[ΑΧ]+')\n",
    "    punct = re.findall(r\"[\\w']+|[.,!?;]\", text)\n",
    "\n",
    "    for token in punct:\n",
    "        output_line = []\n",
    "                    \n",
    "        if bool(re.fullmatch(regex, token)):\n",
    "\n",
    "            output_line.append(\"XAXA\"+\" \") \n",
    "\n",
    "\n",
    "        else:\n",
    "            token=str(token)\n",
    "            output_line.append(token+' ')\n",
    "\n",
    "        r = r + \" \".join(output_line)\n",
    "    return(re.sub(' +', ' ',r))\n",
    "\n",
    "Expicit_NoHate[\"NoLaugh\"]=Expicit_NoHate[\"NoStopwords\"].apply(laugh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate.NoLaugh = Expicit_NoHate.NoLaugh.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"NoLaughTokens\"] = Expicit_NoHate.NoLaugh.apply(tokenize)\n",
    "\n",
    "tokens_no_laugh = []\n",
    "\n",
    "for token in Expicit_NoHate[\"NoLaughTokens\"]:\n",
    "    tokens_no_laugh.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"NoPunctuation\"]=Expicit_NoHate[\"NoLaugh\"].apply(NoPunctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"NoLaughTokens_l\"] = Expicit_NoHate.NoLaugh.str.lower()\n",
    "Expicit_NoHate[\"NoLaughTokens_l\"] = Expicit_NoHate.NoLaughTokens_l.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Expicit_NoHate[\"StemTokens\"] = Expicit_NoHate.Stemming.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed.pkl', 'rb') as handle:\n",
    "    Preprocessed = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines\n",
    "\n",
    "def processing(df):\n",
    "    \n",
    "    new_df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    new_df['Length'] = df.apply(lambda x: len(x))\n",
    "\n",
    "    new_df['Words'] = df.str.split().str.len()\n",
    "\n",
    "    new_df['Avg_word_length'] = df.apply(lambda x: round(np.mean([len(t) for t in x.split(' ')]) if len([len(t) for t in x.split(' ')]) > 0 else 0, 2))\n",
    "\n",
    "    new_df['Punctuation'] =  df.apply(lambda x: sum(1 for c in x if c in string.punctuation))\n",
    "    \n",
    "    new_df['Upper_count'] = df.apply(lambda x: sum(1 for c in x if c.isupper()))\n",
    "    \n",
    "    new_df['Upper_ratio'] = round((new_df['Upper_count'] * 100 )/ new_df['Length'], 1 )\n",
    "    \n",
    "    \n",
    "    return new_df\n",
    "\n",
    "numerical_features = processing(Preprocessed.NoAuthor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed.pkl', 'wb') as handle:\n",
    "    pkl.dump(Expicit_NoHate, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Numerical_Features.pkl', 'wb') as handle:\n",
    "    pkl.dump(numerical_features, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed.pkl', 'rb') as handle:\n",
    "    Preprocessed = pkl.load(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
