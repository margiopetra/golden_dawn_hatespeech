{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Prediction models for imblaced dataset for word embeddings\n",
    "\n",
    "Step1:  Import of the pre-trained word2vec model\n",
    "Step2:  Creation of mean vectors\n",
    "Step3:  Division of data in train and test set\n",
    "Step4:  Perform 10-fold Cross Validation \n",
    "Step4:  Classification (Logistic Regression, CART, Naive Bayes, Linear SVM)\n",
    "Step5:  Creat word2vec model from the dataset\n",
    "Step6:  Creation of mean vectors \n",
    "Step7:  Division of data in train and test set\n",
    "Step8:  Perform 10-fold Cross Validation \n",
    "Step9:  Classification (Logistic Regression, CART, Naive Bayes, Linear SVM)\n",
    "Step10: PCA and plotting of our model\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import pickle as pkl\n",
    "import spacy\n",
    "import el_core_news_sm\n",
    "nlp = el_core_news_sm.load()\n",
    "import string \n",
    "import nltk\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import gensim\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Preprocessed.pkl', 'rb') as handle:\n",
    "    Preprocessed = pkl.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        self.dim = next(iter(word2vec.values())).shape\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                    or [np.zeros(self.dim)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "transfomed_label = encoder.fit_transform(Preprocessed.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_accuracy(X_train, Y_train):\n",
    "    \n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', LinearSVC()))\n",
    "\n",
    "    \n",
    "    results = []\n",
    "    names = []\n",
    "    seed = 7\n",
    "    scoring = 'accuracy'\n",
    "\n",
    "    for name, model in models:\n",
    "        kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "        cv_results = model_selection.cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg,\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(X_train, Y_train, X_test, Y_test):\n",
    "    \n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "    models.append(('CART', DecisionTreeClassifier()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', LinearSVC()))\n",
    "    models.append(('SVM', LinearSVC()))\n",
    "    \n",
    "    for name, model in models:\n",
    "        \n",
    "        print(\"Prediction for \",name)\n",
    "        \n",
    "        model.fit(X_train, Y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        \n",
    "        print(\"Accuracy Score \\n\",accuracy_score(Y_test, predictions),\"\\n\")\n",
    "        print(\"Confusion Matrix \\n\",confusion_matrix(Y_test, predictions),\"\\n\")\n",
    "        print(\"Classification Report \\n\",classification_report(Y_test, predictions),\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://vectors.nlpl.eu/repository/\n",
    "\n",
    "with zipfile.ZipFile(\"46.zip\", \"r\") as archive:\n",
    "      stream = archive.open(\"model.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(stream, binary=False, unicode_errors='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = MeanEmbeddingVectorizer(w2v)\n",
    "\n",
    "transfomed_data = list(Preprocessed.NoLaughTokens_l)\n",
    "\n",
    "a.fit(transfomed_data,transfomed_label)\n",
    "vectorizer = a.transform(transfomed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(vectorizer, Preprocessed.Label, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_accuracy(X_train, Y_train)\n",
    "\n",
    "prediction(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = gensim.models.Word2Vec(Preprocessed[\"NoLaughTokens_l\"], min_count = 1, size = 100, window = 5, sg = 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = dict(zip(model2.wv.index2word, model2.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_m2 = MeanEmbeddingVectorizer(b)\n",
    "\n",
    "transfomed_data = list(Preprocessed.NoLaughTokens_l)\n",
    "\n",
    "mean_m2.fit(transfomed_data,transfomed_label)\n",
    "vectorizer = mean_m2.transform(transfomed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(vectorizer, Preprocessed.Label, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_accuracy(X_train, Y_train)\n",
    "\n",
    "prediction(X_train, Y_train, X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction and plot of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model2[model2.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "\n",
    "fig= plt.figure(figsize=(30,25))\n",
    "\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model2.wv.vocab)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
